# src/eda_cli/core.py
import pandas as pd
from typing import Dict, Any, List

def compute_quality_flags(
    df: pd.DataFrame, 
    min_missing_share: float = 0.3,
    high_cardinality_threshold: int = 50
) -> Dict[str, Any]:
    """
    –í—ã—á–∏—Å–ª—è–µ—Ç —Ñ–ª–∞–≥–∏ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É—á–µ—Ç–æ–º –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.
    
    Args:
        df: DataFrame –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        min_missing_share: –ø–æ—Ä–æ–≥ –¥–ª—è –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –ø—Ä–æ–ø—É—Å–∫–æ–≤
        high_cardinality_threshold: –ø–æ—Ä–æ–≥ –¥–ª—è –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç–∏
    
    Returns:
        –°–ª–æ–≤–∞—Ä—å —Å —Ñ–ª–∞–≥–∞–º–∏ –∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∫–∞—á–µ—Å—Ç–≤–∞
    """
    n_rows, n_cols = df.shape
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_cells = n_rows * n_cols
    missing_cells = df.isnull().sum().sum()
    missing_share = missing_cells / total_cells if total_cells > 0 else 0
    duplicate_rows = df.duplicated().sum()
    
    # –°–ø–∏—Å–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    problematic_missing_cols = []
    for col in df.columns:
        missing_ratio = df[col].isnull().sum() / n_rows
        if missing_ratio > min_missing_share:
            problematic_missing_cols.append({
                'column': col,
                'missing_ratio': missing_ratio
            })
    
    # –ù–û–í–´–ï –≠–í–†–ò–°–¢–ò–ö–ò
    # 1. –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    constant_cols = []
    for col in df.columns:
        if df[col].nunique(dropna=True) == 1:
            constant_cols.append(col)
    
    # 2. –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
    high_cardinality_cols = {}
    for col in df.select_dtypes(include=['object', 'category']).columns:
        unique_count = df[col].nunique()
        if unique_count > high_cardinality_threshold:
            high_cardinality_cols[col] = unique_count
    
    # 3. –ü—Ä–æ–≤–µ—Ä–∫–∞ ID –Ω–∞ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å
    suspicious_id_duplicates = {}
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–ª–æ–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å ID
    possible_id_cols = [col for col in df.columns 
                       if 'id' in col.lower() or col.lower() in ['id', 'index', 'key']]
    
    for col in possible_id_cols:
        duplicate_count = df[col].duplicated().sum()
        if duplicate_count > 0:
            suspicious_id_duplicates[col] = duplicate_count
    
    # –†–∞—Å—á–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞–ª—å–Ω–æ–≥–æ score —Å —É—á–µ—Ç–æ–º –Ω–æ–≤—ã—Ö —Ñ–∞–∫—Ç–æ—Ä–æ–≤
    base_score = 1.0
    
    # –®—Ç—Ä–∞—Ñ—ã
    penalties = {
        'missing': missing_share * 0.5,  # –¥–æ 0.5 –∑–∞ –ø—Ä–æ–ø—É—Å–∫–∏
        'duplicates': min(0.2, duplicate_rows / n_rows * 0.5),
        'constant': len(constant_cols) * 0.1,
        'high_cardinality': len(high_cardinality_cols) * 0.05,
        'id_duplicates': len(suspicious_id_duplicates) * 0.15
    }
    
    quality_score = base_score - sum(penalties.values())
    quality_score = max(0.0, min(1.0, quality_score))
    
    return {
        # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
        'n_rows': n_rows,
        'n_cols': n_cols,
        'missing_share': missing_share,
        'has_missing': missing_cells > 0,
        'duplicate_rows': duplicate_rows,
        'has_duplicates': duplicate_rows > 0,
        'quality_score': quality_score,
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∞–Ω–∞–ª–∏–∑–∞
        'min_missing_share': min_missing_share,
        'high_cardinality_threshold': high_cardinality_threshold,
        
        # –°–ø–∏—Å–∫–∏ –ø—Ä–æ–±–ª–µ–º–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
        'problematic_missing_cols': problematic_missing_cols,
        
        # –ù–æ–≤—ã–µ —Ñ–ª–∞–≥–∏
        'has_constant_columns': len(constant_cols) > 0,
        'constant_columns_list': constant_cols,
        
        'has_high_cardinality_categoricals': len(high_cardinality_cols) > 0,
        'high_cardinality_columns': high_cardinality_cols,
        
        'has_suspicious_id_duplicates': len(suspicious_id_duplicates) > 0,
        'suspicious_id_duplicates': suspicious_id_duplicates,
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        'numeric_columns_count': len(df.select_dtypes(include=['number']).columns),
        'categorical_columns_count': len(df.select_dtypes(include=['object', 'category']).columns),
        'date_columns_count': len(df.select_dtypes(include=['datetime']).columns)
    }

def generate_overview(df: pd.DataFrame) -> str:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –∫—Ä–∞—Ç–∫—É—é —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å–≤–æ–¥–∫—É."""
    flags = compute_quality_flags(df)
    
    overview_lines = [
        "üìä –û–ë–ó–û–† –î–ê–ù–ù–´–•",
        "=" * 50,
        f"–†–∞–∑–º–µ—Ä: {flags['n_rows']} —Å—Ç—Ä–æ–∫ √ó {flags['n_cols']} –∫–æ–ª–æ–Ω–æ–∫",
        f"–ü—Ä–æ–ø—É—Å–∫–∏: {flags['missing_share']:.1%}",
        f"–î—É–±–ª–∏–∫–∞—Ç—ã —Å—Ç—Ä–æ–∫: {flags['duplicate_rows']}",
        f"–ö–∞—á–µ—Å—Ç–≤–æ –¥–∞–Ω–Ω—ã—Ö (score): {flags['quality_score']:.2f}/1.00",
        "",
        "–ü–†–û–ë–õ–ï–ú–ù–´–ï –ö–û–õ–û–ù–ö–ò:"
    ]
    
    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–±–ª–µ–º–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –ø–æ –ø—Ä–æ–ø—É—Å–∫–∞–º
    if flags['problematic_missing_cols']:
        overview_lines.append("  ‚Ä¢ –° –≤—ã—Å–æ–∫–∏–º % –ø—Ä–æ–ø—É—Å–∫–æ–≤ (>30%):")
        for item in flags['problematic_missing_cols']:
            overview_lines.append(f"    - {item['column']}: {item['missing_ratio']:.1%}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    if flags['has_constant_columns']:
        overview_lines.append(f"  ‚Ä¢ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–Ω—ã–µ: {', '.join(flags['constant_columns_list'])}")
    
    # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ–ª–æ–Ω–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é
    if flags['has_high_cardinality_categoricals']:
        overview_lines.append("  ‚Ä¢ –í—ã—Å–æ–∫–∞—è –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
        for col, count in flags['high_cardinality_columns'].items():
            overview_lines.append(f"    - {col}: {count} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π")
    
    return "\n".join(overview_lines)